{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7ef802",
   "metadata": {},
   "source": [
    "\n",
    "# Part 4 — Real‑Time Object Detection (YOLOv8, SSD, OpenCV DNN)\n",
    "\n",
    "**Course:** CV Lab II — Machine Learning with OpenCV  \n",
    "**Author:** _Tsion Bizuayehu  \n",
    "**Last updated:** 2025-09-07 17:48 UTC  \n",
    "\n",
    "In this notebook you will implement and compare **three** real-time object detection pipelines:\n",
    "\n",
    "1. **YOLOv8 (Ultralytics API)** — easiest to start, great accuracy/speed.  \n",
    "2. **MobileNet‑SSD (Caffe) via OpenCV DNN** — lightweight baseline.  \n",
    "3. **YOLOv8 (ONNX) via OpenCV DNN** — framework‑agnostic deployment path.\n",
    "\n",
    "You will run them on images, webcam, and videos; and benchmark FPS.\n",
    "\n",
    "**What you’ll learn**\n",
    "- Set up environment, verify CUDA, and manage models\n",
    "- Run YOLOv8 with Ultralytics in a few lines\n",
    "- Run MobileNet‑SSD with OpenCV DNN\n",
    "- Export YOLOv8 → ONNX and run with OpenCV DNN (post‑processing + NMS)\n",
    "- Measure FPS and save annotated videos\n",
    "\n",
    "_Tip:_ If you’re in Jupyter **Lab/Notebook**, windows opened by OpenCV (`cv2.imshow`) may appear as OS windows.\n",
    "Press **q** in the window to quit loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5778d8",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment Setup\n",
    "\n",
    "Run the next cell to install the required packages.  \n",
    "If you're offline, install these with `pip` first:\n",
    "\n",
    "```txt\n",
    "opencv-python\n",
    "ultralytics\n",
    "numpy\n",
    "onnx\n",
    "onnxruntime  # for ONNX CPU\n",
    "onnxruntime-gpu # (optional) for ONNX CUDA\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bf6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (8.3.195)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: onnx in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (1.19.0)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (1.22.1)\n",
      "Requirement already satisfied: onnxruntime-gpu in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (1.22.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (2.7.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (0.22.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: polars in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (1.33.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.25.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.11.0)\n",
      "Requirement already satisfied: ml_dtypes in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnx) (0.4.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, uncomment to install (internet required)\n",
    "# !pip install -U pip\n",
    "!pip install opencv-python ultralytics numpy onnx onnxruntime onnxruntime-gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61778e",
   "metadata": {},
   "source": [
    "## 1. Imports & Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db82a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7\n",
      "OpenCV: 4.11.0\n",
      "PyTorch: 2.7.0+cpu\n",
      "Ultralytics: 8.3.195\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, time, os, math, json, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch_version = torch.__version__\n",
    "except Exception as e:\n",
    "    torch, torch_version = None, None\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    import ultralytics\n",
    "    yolo_version = ultralytics.__version__\n",
    "except Exception as e:\n",
    "    YOLO, yolo_version = None, None\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "print(\"PyTorch:\", torch_version)\n",
    "print(\"Ultralytics:\", yolo_version)\n",
    "\n",
    "if torch is not None:\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81581cbc",
   "metadata": {},
   "source": [
    "## 2. Utility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b8f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def put_label(img, text, org, color=(0, 255, 0)):\n",
    "    cv2.putText(img, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def fps_counter():\n",
    "    prev = time.time()\n",
    "    while True:\n",
    "        now = time.time()\n",
    "        fps = 1.0 / (now - prev) if now != prev else 0.0\n",
    "        prev = now\n",
    "        yield fps\n",
    "\n",
    "# COCO 80 classes (YOLOv8 default)\n",
    "COCO_NAMES = [\n",
    "    \"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\n",
    "    \"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\n",
    "    \"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\n",
    "    \"skis\",\"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\"surfboard\",\n",
    "    \"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\n",
    "    \"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"couch\",\n",
    "    \"potted plant\",\"bed\",\"dining table\",\"toilet\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\n",
    "    \"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddy bear\",\n",
    "    \"hair drier\",\"toothbrush\"\n",
    "]\n",
    "\n",
    "# Letterbox resize like YOLO for ONNX -> OpenCV pipeline\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True):\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # width, height padding\n",
    "    if auto:\n",
    "        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # 64-pt stride-multiple padding\n",
    "    dw /= 2; dh /= 2\n",
    "\n",
    "    # resize\n",
    "    if shape[::-1] != new_unpad:\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh-0.1)), int(round(dh+0.1))\n",
    "    left, right = int(round(dw-0.1)), int(round(dw+0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return img, r, (dw, dh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572a7d5",
   "metadata": {},
   "source": [
    "\n",
    "## 3. YOLOv8 — Ultralytics API\n",
    "\n",
    "### 3.1 Inference on a single image\n",
    "Put a test image under `data/images/your_image.jpg` and set the path below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbdd22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 1 person, 66.9ms\n",
      "Speed: 38.2ms preprocess, 66.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved: ./outputs/yolov8_image_0.jpg\n"
     ]
    }
   ],
   "source": [
    "ensure_dir(\"./outputs\") # Assuming outputs is in the same directory as the notebook\n",
    "\n",
    "image_path = \"../images/faces/face_sample.jpg\"  # Corrected path to the image\n",
    "model_name = \"yolov8n.pt\"                       # 'n' is fast; try 's','m' for accuracy\n",
    "\n",
    "if YOLO is None:\n",
    "    raise RuntimeError(\"Ultralytics not installed. Run the pip cell above.\")\n",
    "\n",
    "model = YOLO(model_name)  # auto-downloads weights on first use\n",
    "img = cv2.imread(image_path)\n",
    "assert img is not None, f\"Image not found at {image_path}. Place an image and re-run.\"\n",
    "\n",
    "results = model(img, conf=0.5)\n",
    "# Plot and save\n",
    "for i, r in enumerate(results):\n",
    "    plotted = r.plot()\n",
    "    out_path = f\"./outputs/yolov8_image_{i}.jpg\"\n",
    "    cv2.imwrite(out_path, plotted)\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8159c16",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Webcam / Video inference (press **q** to quit)\n",
    "\n",
    "- Set `source = 0` for default webcam, or provide a video path.\n",
    "- Set `save_vid = True` to write an output video under `outputs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f180d23-3739-44ff-a643-2382f22fcfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement kaggle-hub (from versions: none)\n",
      "ERROR: No matching distribution found for kaggle-hub\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cdd9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototxt not found in project folder. Searching OpenCV installation...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "models/ssd/MobileNetSSD_deploy.prototxt not found.\nPlease manually download it and place it in models/ssd/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopied prototxt from OpenCV samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback_path)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproto_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease manually download it and place it in models/ssd/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Verify caffemodel exists\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(caffemodel_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaffemodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: models/ssd/MobileNetSSD_deploy.prototxt not found.\nPlease manually download it and place it in models/ssd/"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# ----------------------\n",
    "# Paths to model files\n",
    "# ----------------------\n",
    "proto_path = \"models/ssd/MobileNetSSD_deploy.prototxt\"\n",
    "caffemodel_path = \"models/ssd/MobileNetSSD_deploy.caffemodel\"\n",
    "\n",
    "# ----------------------\n",
    "# Auto-download prototxt if missing\n",
    "# ----------------------\n",
    "if not Path(proto_path).exists():\n",
    "    print(\"Prototxt not found. Downloading...\")\n",
    "    proto_url = \"https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/MobileNetSSD_deploy.prototxt\"\n",
    "    Path(proto_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(proto_url, proto_path)\n",
    "    print(\"Downloaded:\", proto_path)\n",
    "\n",
    "# ----------------------\n",
    "# Verify caffemodel exists\n",
    "# ----------------------\n",
    "assert Path(caffemodel_path).exists(), f\"{caffemodel_path} not found!\"\n",
    "\n",
    "# ----------------------\n",
    "# VOC classes\n",
    "# ----------------------\n",
    "VOC_CLASSES = [\n",
    "    \"background\",\"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\"bus\",\"car\",\"cat\",\"chair\",\n",
    "    \"cow\",\"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\"pottedplant\",\"sheep\",\"sofa\",\n",
    "    \"train\",\"tvmonitor\"\n",
    "]\n",
    "\n",
    "# ----------------------\n",
    "# Helper function for labels\n",
    "# ----------------------\n",
    "def put_label(img, text, org, color=(0, 255, 0)):\n",
    "    cv2.putText(img, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "# ----------------------\n",
    "# Load pretrained SSD model\n",
    "# ----------------------\n",
    "net = cv2.dnn.readNetFromCaffe(proto_path, caffemodel_path)\n",
    "\n",
    "# ----------------------\n",
    "# Start webcam\n",
    "# ----------------------\n",
    "cap = cv2.VideoCapture(0)  # 0 = default camera\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = float(detections[0, 0, i, 2])\n",
    "        if confidence > 0.5:\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw label\n",
    "            label = f\"{VOC_CLASSES[idx]}: {confidence:.2f}\"\n",
    "            put_label(frame, label, (x1, max(0, y1 - 6)))\n",
    "\n",
    "    cv2.imshow(\"SSD Detection\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684cac3d-b3ad-43df-85e6-21468b22a524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\tsion\\Downloads\\cv-lab2-opencv-ml\\part4-real-time-detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d3723aa-7b9c-4edb-97c4-2822bc04a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototxt exists: False\n",
      "Caffemodel exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"Prototxt exists:\", Path(\"models/ssd/MobileNetSSD_deploy.prototxt\").exists())\n",
    "print(\"Caffemodel exists:\", Path(\"models/ssd/MobileNetSSD_deploy.caffemodel\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ffbca",
   "metadata": {},
   "source": [
    "### 4.1 SSD — Image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proto = \"models/ssd/MobileNetSSD_deploy.prototxt\"\n",
    "caffe = \"models/ssd/MobileNetSSD_deploy.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(proto, caffe)\n",
    "\n",
    "img_path = \"../images/faces/face_sample.jpg\"  # Corrected path to the image\n",
    "\n",
    "img = cv2.imread(img_path); assert img is not None, \"Place an image at data/images/test.jpg\"\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "net.setInput(blob)\n",
    "dets = net.forward()\n",
    "\n",
    "for i in range(dets.shape[2]):\n",
    "    conf = float(dets[0, 0, i, 2])\n",
    "    if conf >= 0.5:\n",
    "        cls_id = int(dets[0, 0, i, 1])\n",
    "        x1, y1, x2, y2 = (dets[0, 0, i, 3:7] * np.array([w, h, w, h])).astype(int)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label = f\"{VOC_CLASSES[cls_id]} {conf:.2f}\"\n",
    "        put_label(img, label, (x1, max(0, y1-6)))\n",
    "\n",
    "out_path = \"outputs/ssd_image.jpg\"\n",
    "ensure_dir(\"outputs\")\n",
    "cv2.imwrite(out_path, img)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e2e16-34ab-47f3-9a65-22354a85a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "print(Path(\"models/ssd/MobileNetSSD_deploy.prototxt\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b3bd0",
   "metadata": {},
   "source": [
    "### 4.2 SSD — Webcam/Video (press **q** to quit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57147829",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "models/ssd/MobileNetSSD_deploy.prototxt not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples/test_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your actual image\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Verify files exist\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(proto_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproto_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(caffemodel_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaffemodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(image_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: models/ssd/MobileNetSSD_deploy.prototxt not found!"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------\n",
    "# Paths to model files\n",
    "# ----------------------\n",
    "proto_path = \"models/ssd/MobileNetSSD_deploy.prototxt\"\n",
    "caffemodel_path = \"models/ssd/MobileNetSSD_deploy.caffemodel\"\n",
    "image_path = \"samples/test_image.jpg\"  # Replace with your actual image\n",
    "\n",
    "# ----------------------\n",
    "# Verify files exist\n",
    "# ----------------------\n",
    "assert Path(proto_path).exists(), f\"{proto_path} not found!\"\n",
    "assert Path(caffemodel_path).exists(), f\"{caffemodel_path} not found!\"\n",
    "assert Path(image_path).exists(), f\"{image_path} not found!\"\n",
    "\n",
    "# ----------------------\n",
    "# VOC classes\n",
    "# ----------------------\n",
    "VOC_CLASSES = [\n",
    "    \"background\",\"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\"bus\",\"car\",\"cat\",\"chair\",\n",
    "    \"cow\",\"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\"pottedplant\",\"sheep\",\"sofa\",\n",
    "    \"train\",\"tvmonitor\"\n",
    "]\n",
    "\n",
    "# ----------------------\n",
    "# Helper function for labels\n",
    "# ----------------------\n",
    "def put_label(img, text, org, color=(0, 255, 0)):\n",
    "    cv2.putText(img, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "# ----------------------\n",
    "# Load image and model\n",
    "# ----------------------\n",
    "frame = cv2.imread(image_path)\n",
    "assert frame is not None, f\"Failed to load image: {image_path}\"\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(proto_path, caffemodel_path)\n",
    "\n",
    "# ----------------------\n",
    "# Run detection\n",
    "# ----------------------\n",
    "h, w = frame.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5)\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = float(detections[0, 0, i, 2])\n",
    "    if confidence > 0.5:\n",
    "        idx = int(detections[0, 0, i, 1])\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label = f\"{VOC_CLASSES[idx]}: {confidence:.2f}\"\n",
    "        put_label(frame, label, (x1, max(0, y1 - 6)))\n",
    "\n",
    "# ----------------------\n",
    "# Show and save result\n",
    "# ----------------------\n",
    "cv2.imshow(\"SSD Detection\", frame)\n",
    "cv2.imwrite(\"outputs/ssd_result.jpg\", frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Detection complete. Saved to outputs/ssd_result.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3c151",
   "metadata": {},
   "source": [
    "\n",
    "## 5. YOLOv8 → ONNX (export) and OpenCV DNN\n",
    "\n",
    "### 5.1 Export weights to ONNX\n",
    "Run this once to generate `yolov8n.onnx` in `models/yolo/`. You need Ultralytics installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9a531bb-2c81-4ce2-95a5-c1052f7f6f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.195  Python-3.12.7 torch-2.7.0+cpu CPU (11th Gen Intel Core i7-1165G7 @ 2.80GHz)\n",
      " ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxslim>=0.1.67'] not found, attempting AutoUpdate...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnxslim>=0.1.67\n",
      "  Downloading onnxslim-0.1.67-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: onnx in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxslim>=0.1.67) (1.19.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxslim>=0.1.67) (1.14.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxslim>=0.1.67) (24.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxslim>=0.1.67) (0.4.6)\n",
      "Requirement already satisfied: ml_dtypes in c:\\users\\tsion\\appdata\\roaming\\python\\python312\\site-packages (from onnxslim>=0.1.67) (0.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->onnxslim>=0.1.67) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from ml_dtypes->onnxslim>=0.1.67) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx->onnxslim>=0.1.67) (4.25.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx->onnxslim>=0.1.67) (4.11.0)\n",
      "Downloading onnxslim-0.1.67-py3-none-any.whl (164 kB)\n",
      "Installing collected packages: onnxslim\n",
      "Successfully installed onnxslim-0.1.67\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  7.8s\n",
      "WARNING \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "ERROR \u001b[34m\u001b[1mONNX:\u001b[0m export failure 8.3s: module 'ml_dtypes' has no attribute 'float4_e2m1fn'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ml_dtypes' has no attribute 'float4_e2m1fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load and export YOLOv8n to ONNX format\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# You can also use yolov8s.pt or yolov8m.pt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:736\u001b[0m, in \u001b[0;36mModel.export\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m custom \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    734\u001b[0m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[0;32m    735\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Exporter(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\exporter.py:493\u001b[0m, in \u001b[0;36mExporter.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    491\u001b[0m     f[\u001b[38;5;241m1\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_engine(dla\u001b[38;5;241m=\u001b[39mdla)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m onnx:  \u001b[38;5;66;03m# ONNX\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     f[\u001b[38;5;241m2\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_onnx()\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xml:  \u001b[38;5;66;03m# OpenVINO\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     f[\u001b[38;5;241m3\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_openvino()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\exporter.py:202\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    201\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export failure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\exporter.py:197\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Profile() \u001b[38;5;28;01mas\u001b[39;00m dt:\n\u001b[1;32m--> 197\u001b[0m         f, model \u001b[38;5;241m=\u001b[39m inner_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    198\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export success ✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size(f)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\exporter.py:592\u001b[0m, in \u001b[0;36mExporter.export_onnx\u001b[1;34m(self, prefix)\u001b[0m\n\u001b[0;32m    590\u001b[0m     requirements \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxslim>=0.1.67\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxruntime\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    591\u001b[0m check_requirements(requirements)\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    594\u001b[0m opset_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mopset \u001b[38;5;129;01mor\u001b[39;00m get_latest_opset()\n\u001b[0;32m    595\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starting export with onnx \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnx\\__init__.py:131\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Import common subpackages so they're available when you 'import onnx'\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     checker,\n\u001b[0;32m    133\u001b[0m     compose,\n\u001b[0;32m    134\u001b[0m     defs,\n\u001b[0;32m    135\u001b[0m     gen_proto,\n\u001b[0;32m    136\u001b[0m     helper,\n\u001b[0;32m    137\u001b[0m     hub,\n\u001b[0;32m    138\u001b[0m     numpy_helper,\n\u001b[0;32m    139\u001b[0m     parser,\n\u001b[0;32m    140\u001b[0m     printer,\n\u001b[0;32m    141\u001b[0m     shape_inference,\n\u001b[0;32m    142\u001b[0m     utils,\n\u001b[0;32m    143\u001b[0m     version_converter,\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    146\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mversion\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Supported model formats that can be loaded from and saved to\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# The literals are formats with built-in support. But we also allow users to\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# register their own formats. So we allow str as well.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnx\\compose.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     AttributeProto,\n\u001b[0;32m     10\u001b[0m     GraphProto,\n\u001b[0;32m     11\u001b[0m     ModelProto,\n\u001b[0;32m     12\u001b[0m     TensorProto,\n\u001b[0;32m     13\u001b[0m     checker,\n\u001b[0;32m     14\u001b[0m     helper,\n\u001b[0;32m     15\u001b[0m     utils,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MutableMapping\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnx\\helper.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mapping, defs, subbyte\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx_data_pb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MapProto, OptionalProto, SequenceProto\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx_pb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     AttributeProto,\n\u001b[0;32m     25\u001b[0m     FunctionProto,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     ValueInfoProto,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnx\\_mapping.py:104\u001b[0m\n\u001b[0;32m     17\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# tensor_dtype: (numpy type, storage type, string name)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# The storage type is the type used to store the tensor in the *_data field of\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# a TensorProto. All available fields are float_data, int32_data, int64_data,\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# string_data, uint64_data and double_data.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m TENSOR_TYPE_MAP: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, TensorDtypeMap] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT): TensorDtypeMap(\n\u001b[0;32m     26\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     ),\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT8): TensorDtypeMap(\n\u001b[0;32m     29\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.UINT8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m     ),\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT8): TensorDtypeMap(\n\u001b[0;32m     32\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.INT8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m     ),\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT16): TensorDtypeMap(\n\u001b[0;32m     35\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.UINT16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     ),\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT16): TensorDtypeMap(\n\u001b[0;32m     38\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.INT16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m     ),\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32): TensorDtypeMap(\n\u001b[0;32m     41\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.INT32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     ),\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT64): TensorDtypeMap(\n\u001b[0;32m     44\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT64), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.INT64\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m     ),\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mBOOL): TensorDtypeMap(\n\u001b[0;32m     47\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m     ),\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT16): TensorDtypeMap(\n\u001b[0;32m     50\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     ),\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mBFLOAT16): TensorDtypeMap(\n\u001b[0;32m     53\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mbfloat16),\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.BFLOAT16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m     ),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mDOUBLE): TensorDtypeMap(\n\u001b[0;32m     58\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mDOUBLE), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.DOUBLE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m     ),\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mCOMPLEX64): TensorDtypeMap(\n\u001b[0;32m     61\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex64\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.COMPLEX64\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     ),\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mCOMPLEX128): TensorDtypeMap(\n\u001b[0;32m     64\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex128\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mDOUBLE),\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.COMPLEX128\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m     ),\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT32): TensorDtypeMap(\n\u001b[0;32m     69\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint32\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT64), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.UINT32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     ),\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT64): TensorDtypeMap(\n\u001b[0;32m     72\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint64\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT64), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.UINT64\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     ),\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mSTRING): TensorDtypeMap(\n\u001b[0;32m     75\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mSTRING), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     ),\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E4M3FN): TensorDtypeMap(\n\u001b[0;32m     78\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e4m3fn),\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT8E4M3FN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     ),\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E4M3FNUZ): TensorDtypeMap(\n\u001b[0;32m     83\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e4m3fnuz),\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT8E4M3FNUZ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m     ),\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E5M2): TensorDtypeMap(\n\u001b[0;32m     88\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e5m2),\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT8E5M2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     91\u001b[0m     ),\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E5M2FNUZ): TensorDtypeMap(\n\u001b[0;32m     93\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e5m2fnuz),\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT8E5M2FNUZ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     96\u001b[0m     ),\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mUINT4): TensorDtypeMap(\n\u001b[0;32m     98\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39muint4), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.UINT4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     ),\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT4): TensorDtypeMap(\n\u001b[0;32m    101\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mint4), \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.INT4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     ),\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT4E2M1): TensorDtypeMap(\n\u001b[1;32m--> 104\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat4_e2m1fn),\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT4E2M1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     ),\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E8M0): TensorDtypeMap(\n\u001b[0;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mdtype(ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e8m0fnu),\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28mint\u001b[39m(TensorProto\u001b[38;5;241m.\u001b[39mINT32),\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorProto.FLOAT8E8M0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m     ),\n\u001b[0;32m    113\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ml_dtypes' has no attribute 'float4_e2m1fn'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load and export YOLOv8n to ONNX format\n",
    "model = YOLO(\"yolov8n.pt\")  # You can also use yolov8s.pt or yolov8m.pt\n",
    "model.export(format=\"onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e24c37c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "models/yolov8/yolov8n.onnx not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images/faces/face_sample.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Corrected path to the image\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Verify files\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(onnx_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(image_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# COCO class names\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: models/yolov8/yolov8n.onnx not found!"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------\n",
    "# Paths\n",
    "# ----------------------\n",
    "onnx_path = \"models/yolov8/yolov8n.onnx\"\n",
    "image_path = \"../images/faces/face_sample.jpg\"  # Corrected path to the image\n",
    "\n",
    "# ----------------------\n",
    "# Verify files\n",
    "# ----------------------\n",
    "assert Path(onnx_path).exists(), f\"{onnx_path} not found!\"\n",
    "assert Path(image_path).exists(), f\"{image_path} not found!\"\n",
    "\n",
    "# ----------------------\n",
    "# COCO class names\n",
    "# ----------------------\n",
    "COCO_CLASSES = [\n",
    "    \"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\n",
    "    \"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\n",
    "    \"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\n",
    "    \"skis\",\"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\"surfboard\",\n",
    "    \"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\n",
    "    \"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"couch\",\n",
    "    \"potted plant\",\"bed\",\"dining table\",\"toilet\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\n",
    "    \"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddy bear\",\n",
    "    \"hair drier\",\"toothbrush\"\n",
    "]\n",
    "\n",
    "def put_label(img, text, org, color=(0, 255, 0)):\n",
    "    cv2.putText(img, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "# ----------------------\n",
    "# Load model and image\n",
    "# ----------------------\n",
    "net = cv2.dnn.readNetFromONNX(str(onnx_path))\n",
    "frame = cv2.imread(image_path)\n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "# ----------------------\n",
    "# Run inference\n",
    "# ----------------------\n",
    "blob = cv2.dnn.blobFromImage(frame, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "outputs = net.forward()\n",
    "\n",
    "for det in outputs[0]:\n",
    "    conf = det[4]\n",
    "    if conf < 0.5:\n",
    "        continue\n",
    "    scores = det[5:]\n",
    "    class_id = np.argmax(scores)\n",
    "    score = scores[class_id]\n",
    "    if score < 0.5:\n",
    "        continue\n",
    "\n",
    "    cx, cy, bw, bh = det[:4]\n",
    "    x1 = int((cx - bw / 2) * w / 640)\n",
    "    y1 = int((cy - bh / 2) * h / 640)\n",
    "    x2 = int((cx + bw / 2) * w / 640)\n",
    "    y2 = int((cy + bh / 2) * h / 640)\n",
    "\n",
    "    label = f\"{COCO_CLASSES[class_id]}: {score:.2f}\"\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    put_label(frame, label, (x1, max(0, y1 - 10)))\n",
    "\n",
    "# ----------------------\n",
    "# Show and save result\n",
    "# ----------------------\n",
    "cv2.imshow(\"YOLOv8 ONNX Detection\", frame)\n",
    "cv2.imwrite(\"outputs/yolov8_result.jpg\", frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Detection complete. Saved to outputs/yolov8_result.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f949b9",
   "metadata": {},
   "source": [
    "### 5.2 OpenCV DNN inference (ONNX) — helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c12394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yolo_postprocess(outputs: np.ndarray, img_shape, conf_thres=0.25, iou_thres=0.45):\n",
    "    # outputs: (batch, num, 85) or (num, 85); we assume (1, N, 85)\n",
    "    if outputs.ndim == 3:\n",
    "        outputs = outputs[0]  # (N, 85)\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    h, w = img_shape[:2]\n",
    "    for i in range(outputs.shape[0]):\n",
    "        row = outputs[i]\n",
    "        obj = row[4] if row.shape[0] >= 6 else 1.0  # compatibility\n",
    "        class_scores = row[5:]\n",
    "        if class_scores.size == 0:\n",
    "            # Some exports pack x,y,w,h + class confs (no obj)\n",
    "            class_scores = row[4:]\n",
    "            obj = 1.0\n",
    "        cls_id = int(np.argmax(class_scores))\n",
    "        conf = class_scores[cls_id] * obj\n",
    "        if conf >= conf_thres:\n",
    "            # xywh -> xyxy in original image scale is handled outside; here we keep raw\n",
    "            boxes.append(row[:4])\n",
    "            scores.append(float(conf))\n",
    "            class_ids.append(cls_id)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return [], [], []\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # NMS pre: convert from cx,cy,w,h to x1,y1,x2,y2 (in the resized image space 640x640)\n",
    "    cx, cy, bw, bh = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
    "    x1 = cx - bw/2; y1 = cy - bh/2; x2 = cx + bw/2; y2 = cy + bh/2\n",
    "    boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)\n",
    "\n",
    "    # OpenCV NMS\n",
    "    idxs = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes_xyxy.tolist(),\n",
    "        scores=scores.tolist(),\n",
    "        score_threshold=conf_thres,\n",
    "        nms_threshold=iou_thres\n",
    "    )\n",
    "    idxs = idxs.flatten().tolist() if len(idxs) > 0 else []\n",
    "    return boxes_xyxy[idxs], scores[idxs].tolist(), [class_ids[i] for i in idxs]\n",
    "\n",
    "def scale_coords(resized_shape, boxes_xyxy, original_shape, ratio_pad):\n",
    "    # Map boxes from letterboxed image space back to original image space\n",
    "    (_, _), (dw, dh) = ((0,0), ratio_pad)\n",
    "    gain = min(resized_shape[0] / original_shape[0], resized_shape[1] / original_shape[1])\n",
    "    boxes = boxes_xyxy.copy()\n",
    "    boxes[:, [0,2]] -= dw*2\n",
    "    boxes[:, [1,3]] -= dh*2\n",
    "    boxes[:, :4] /= gain\n",
    "    # clip\n",
    "    h, w = original_shape[:2]\n",
    "    boxes[:, [0,2]] = boxes[:, [0,2]].clip(0, w-1)\n",
    "    boxes[:, [1,3]] = boxes[:, [1,3]].clip(0, h-1)\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996ee09",
   "metadata": {},
   "source": [
    "### 5.3 ONNX — Image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_path = \"models/yolo/yolov8n.onnx\"\n",
    "net = cv2.dnn.readNetFromONNX(onnx_path)\n",
    "\n",
    "img_path = \"data/images/test.jpg\"\n",
    "img0 = cv2.imread(img_path); assert img0 is not None, \"Place an image at data/images/test.jpg\"\n",
    "img, r, (dw, dh) = letterbox(img0, (640, 640))\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "out = net.forward()  # shape: (1, N, 85) for YOLOv8\n",
    "\n",
    "boxes_xyxy, scores, class_ids = yolo_postprocess(out, img.shape, conf_thres=0.35, iou_thres=0.5)\n",
    "if len(boxes_xyxy):\n",
    "    # scale back to original image\n",
    "    boxes_xyxy = scale_coords((640,640), boxes_xyxy, img0.shape, (dw, dh)).astype(int)\n",
    "    for (x1,y1,x2,y2), s, cid in zip(boxes_xyxy, scores, class_ids):\n",
    "        cv2.rectangle(img0, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "        cls = COCO_NAMES[cid] if cid < len(COCO_NAMES) else str(cid)\n",
    "        put_label(img0, f\"{cls} {s:.2f}\", (x1, max(0, y1-6)))\n",
    "\n",
    "ensure_dir(\"outputs\")\n",
    "out_path = \"outputs/yolov8_onnx_image.jpg\"\n",
    "cv2.imwrite(out_path, img0)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffedb5",
   "metadata": {},
   "source": [
    "### 5.4 ONNX — Webcam/Video (press **q** to quit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e723690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source = 0  # webcam or path to mp4\n",
    "cap = cv2.VideoCapture(source)\n",
    "assert cap.isOpened(), f\"Cannot open source: {source}\"\n",
    "out_path = \"outputs/yolov8_onnx_out.mp4\"\n",
    "\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 640\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 480\n",
    "fps_out = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "\n",
    "net = cv2.dnn.readNetFromONNX(\"models/yolo/yolov8n.onnx\")\n",
    "\n",
    "writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps_out, (w, h))\n",
    "fps_gen = fps_counter()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    img, r, (dw, dh) = letterbox(frame, (640, 640))\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()\n",
    "\n",
    "    boxes_xyxy, scores, class_ids = yolo_postprocess(out, img.shape, conf_thres=0.35, iou_thres=0.5)\n",
    "    if len(boxes_xyxy):\n",
    "        boxes_xyxy = scale_coords((640,640), boxes_xyxy, frame.shape, (dw, dh)).astype(int)\n",
    "        for (x1,y1,x2,y2), s, cid in zip(boxes_xyxy, scores, class_ids):\n",
    "            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "            cls = COCO_NAMES[cid] if cid < len(COCO_NAMES) else str(cid)\n",
    "            put_label(frame, f\"{cls} {s:.2f}\", (x1, max(0, y1-6)))\n",
    "\n",
    "    fps = next(fps_gen)\n",
    "    put_label(frame, f\"FPS: {fps:.1f}\", (10, 30), (0, 255, 255))\n",
    "\n",
    "    writer.write(frame)\n",
    "    cv2.imshow(\"YOLOv8 ONNX (OpenCV DNN)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release(); writer.release(); cv2.destroyAllWindows()\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed556c7",
   "metadata": {},
   "source": [
    "## 6. Simple FPS Benchmark (Image batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "# Put a few images under data/images/\n",
    "imgs = [cv2.imread(p) for p in sorted(glob.glob(\"data/images/*\"))[:8]]\n",
    "imgs = [im for im in imgs if im is not None]\n",
    "assert imgs, \"Add some images into data/images/\"\n",
    "\n",
    "# YOLOv8 (Ultralytics) single-image timing\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "t0 = time.time()\n",
    "for im in imgs:\n",
    "    _ = model.predict(source=im, conf=0.5, verbose=False)\n",
    "t1 = time.time()\n",
    "print(f\"YOLOv8 Ultralytics: {(len(imgs)/(t1-t0)):.2f} FPS (images/sec)\")\n",
    "\n",
    "# SSD (OpenCV) timing\n",
    "net = cv2.dnn.readNetFromCaffe(\"models/ssd/MobileNetSSD_deploy.prototxt\",\n",
    "                               \"models/ssd/MobileNetSSD_deploy.caffemodel\")\n",
    "t0 = time.time()\n",
    "for im in imgs:\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(im, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob); _ = net.forward()\n",
    "t1 = time.time()\n",
    "print(f\"MobileNet-SSD DNN: {(len(imgs)/(t1-t0)):.2f} FPS (images/sec)\")\n",
    "\n",
    "# YOLOv8 ONNX (OpenCV DNN) timing\n",
    "net = cv2.dnn.readNetFromONNX(\"models/yolo/yolov8n.onnx\")\n",
    "t0 = time.time()\n",
    "for im in imgs:\n",
    "    img, r, (dw, dh) = letterbox(im, (640, 640))\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "    net.setInput(blob); _ = net.forward()\n",
    "t1 = time.time()\n",
    "print(f\"YOLOv8 ONNX (DNN): {(len(imgs)/(t1-t0)):.2f} FPS (images/sec)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbef1fb",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Wrap‑Up & Next Steps\n",
    "\n",
    "- Try different YOLOv8 sizes: `yolov8s.pt`, `yolov8m.pt` for accuracy vs speed\n",
    "- Replace COCO names with your dataset and fine‑tune a custom model (Ultralytics `model.train(...)`)\n",
    "- Compare FPS on CPU vs GPU (Torch vs OpenCV DNN vs ONNX Runtime)\n",
    "\n",
    "**Suggested Exercises**\n",
    "1. Replace YOLOv8 with YOLOv5 or YOLOv7 and compare FPS/accuracy.\n",
    "2. Benchmark SSD vs YOLOv8 on your machine (CPU/GPU) and record results in a table.\n",
    "3. Train a YOLO model on a small custom dataset and test in real‑time.\n",
    "4. Use OpenCV DNN to run COCO-trained ONNX **without** Ultralytics API (done above!).\n",
    "\n",
    "**Notes**\n",
    "- This notebook follows the CV Lab manual’s Chapter 11 outline (Real-Time Object Detection).  \n",
    "- If `cv2.imshow` windows do not appear in your environment, run in a local Python script or enable GUI backend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9917de3-76c5-48fe-966d-3da691083413",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
